{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–µ—Ç—Ä–µ–π–Ω–∞ LLM\n",
        "\n",
        "**–¢–µ—Å—Ç–æ–≤–æ–µ –∑–∞–¥–∞–Ω–∏–µ DS 2026 ‚Äî –¢–æ—á–∫–∞ –±–∞–Ω–∫**\n",
        "\n",
        "## –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏\n",
        "\n",
        "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å ¬´–≥—Ä—è–∑–Ω—ã–π¬ª –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø—Ä–µ—Ç—Ä–µ–π–Ω–∞ LLM. –¢–µ–∫—Å—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è –ø–æ —Ç—Ä—ë–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è 0/1):\n",
        "\n",
        "| –ö—Ä–∏—Ç–µ—Ä–∏–π | –û–ø–∏—Å–∞–Ω–∏–µ | 1 | 0 |\n",
        "|----------|----------|---|---|\n",
        "| **Integrity** (–µ–¥–∏–Ω—Å—Ç–≤–æ —Ç–µ–º—ã) | –ú–æ–∂–Ω–æ –ª–∏ –æ–∑–∞–≥–ª–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –∏ –≤—ã–¥–µ–ª–∏—Ç—å –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å? | –î–∞ (—Å—Ç–∞—Ç—å–∏) | –ù–µ—Ç (—Å–ø–∏—Å–∫–∏, –∫–∞—Ç–∞–ª–æ–≥–∏) |\n",
        "| **Factuality** (—Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–Ω–æ—Å—Ç—å) | –°–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —Ñ–∞–∫—Ç—ã –æ –º–∏—Ä–µ? | –î–∞ (—Å—Ç–∞—Ç—å–∏, –æ–ø–∏—Å–∞–Ω–∏—è) | –ù–µ—Ç (–º–Ω–µ–Ω–∏—è, –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è) |\n",
        "| **Truthfulness** (–ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å) | –ù–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç common-sense –∑–Ω–∞–Ω–∏—è–º? | –î–∞ | –ù–µ—Ç (—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞, –≤—ã–º—ã—Å–µ–ª) |\n",
        "\n",
        "**–ú–µ—Ç—Ä–∏–∫–∞:** —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ F1 –ø–æ –∫–∞–∂–¥–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é. –ü—Ä–∏–º–µ—Ä—ã —Å –º–µ—Ç–∫–æ–π 0.5 –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è –∏–∑ —Ä–∞—Å—á—ë—Ç–∞."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7e9979",
      "metadata": {},
      "source": [
        "## –ü–æ–¥—Ö–æ–¥ –∏ –≥–∏–ø–æ—Ç–µ–∑—ã\n",
        "\n",
        "### –ö–∞–∫–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã –ø—Ä–æ–≤–µ—Ä—è–ª–∏—Å—å –∏ —á—Ç–æ –¥–∞–ª–∏\n",
        "\n",
        "1. **Rule-based baseline** ‚Äî —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–ª–∏–Ω–µ —Å—Ç—Ä–æ–∫, –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –¥–æ–ª–µ ¬´—Å–ø–∏—Å–æ—á–Ω—ã—Ö¬ª –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤. –î–∞—ë—Ç –±—ã—Å—Ç—Ä—ã–π –æ—Ä–∏–µ–Ω—Ç–∏—Ä –∏ –Ω–µ–ø–ª–æ—Ö–æ –ª–æ–≤–∏—Ç –æ—á–µ–≤–∏–¥–Ω—ã–π –º—É—Å–æ—Ä (–∫–∞—Ç–∞–ª–æ–≥–∏, —Å–ø–∏—Å–∫–∏ –≥–æ—Ä–æ–¥–æ–≤), –Ω–æ –Ω–µ —Ä–∞–∑–ª–∏—á–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–ª–æ–∂–Ω—ã–µ —Å–ª—É—á–∞–∏. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –∫–∞–∫ sanity check.\n",
        "\n",
        "2. **ML –Ω–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (sklearn, XGBoost)** ‚Äî –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –¥–æ–ª—è —Å—Ç–æ–ø-—Å–ª–æ–≤, –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Ç.–ø. Baseline –≤—ã—à–µ rule-based, –Ω–æ —É–ø–∏—Ä–∞–µ—Ç—Å—è –≤ –ø–æ—Ç–æ–ª–æ–∫: —Ñ–∏—á–∏ –Ω–µ –æ—Ç—Ä–∞–∂–∞—é—Ç —Å–º—ã—Å–ª —Ç–µ–∫—Å—Ç–∞. –ü–æ–¥—Ö–æ–¥–∏—Ç –∫–∞–∫ –±—ã—Å—Ç—Ä—ã–π —Ñ–∏–ª—å—Ç—Ä, –Ω–æ –¥–ª—è –∑–∞–¥–∞—á–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–µ—Ç—Ä–µ–π–Ω-–¥–∞–Ω–Ω—ã—Ö —ç—Ç–æ–≥–æ –º–∞–ª–æ.\n",
        "\n",
        "3. **–û—Ç–¥–µ–ª—å–Ω—ã–µ BERT-–º–æ–¥–µ–ª–∏ –Ω–∞ –∫–∞–∂–¥—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π** ‚Äî —Ç—Ä–∏ –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–π. –û–∫–∞–∑–∞–ª–æ—Å—å —Ö—É–∂–µ, —á–µ–º multi-task: –∫–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—â–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –û—Ç–¥–µ–ª—å–Ω—ã–µ ¬´–≥–æ–ª–æ–≤—ã¬ª –Ω–∞ –æ–±—â–µ–º —ç–Ω–∫–æ–¥–µ—Ä-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –¥–∞–ª–∏ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.\n",
        "\n",
        "4. **Multi-Task BERT** ‚Äî –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å, —Ç—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –≥–æ–ª–æ–≤—ã. –û–±—â–∏–π —ç–Ω–∫–æ–¥–µ—Ä —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º, –ø–æ–ª–µ–∑–Ω—ã–º –¥–ª—è –≤—Å–µ—Ö —Ç—Ä—ë—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤. –õ—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –¥–∞—ë—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π —Å—Ä–µ–¥–Ω–∏–π F1.\n",
        "\n",
        "5. **–†–∞–∑–º–æ—Ä–æ–∑–∫–∞ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤** ‚Äî –ø—Ä–∏ –ø–æ–ª–Ω–æ–º fine-tune BERT –±—ã—Å—Ç—Ä–æ –ø–µ—Ä–µ–æ–±—É—á–∞–ª—Å—è. –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 2 —Å–ª–æ—ë–≤ + pooler + –≥–æ–ª–æ–≤—ã –¥–∞–ª–æ –ª—É—á—à–∏–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é.\n",
        "\n",
        "---\n",
        "\n",
        "### –ü–æ—á–µ–º—É –Ω–µ–ª—å–∑—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å F1 –¥–ª—è –∫–ª–∞—Å—Å–∞ 1\n",
        "\n",
        "–ö–ª–∞—Å—Å **1** (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç) ‚Äî –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –∫–ª–∞—Å—Å –≤ –≤—ã–±–æ—Ä–∫–µ. –ï—Å–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å F1 –ø–æ –∫–ª–∞—Å—Å—É 1, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ —Å–∫–æ—Ä, –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è 1: –º–Ω–æ–≥–æ –∏—Å—Ç–∏–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö, –º–∞–ª–æ –ª–æ–∂–Ω–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏ —ç—Ç–æ —Ç—Ä–∏–≤–∏–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è ¬´–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –º–∞–∂–æ—Ä–Ω—ã–π –∫–ª–∞—Å—Å¬ª, –∏ –æ–Ω–∞ –Ω–µ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º—É—Å–æ—Ä–∞.\n",
        "\n",
        "–ö–ª–∞—Å—Å **0** (–º—É—Å–æ—Ä, –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ) ‚Äî —Ä–µ–¥–∫–∏–π, –Ω–æ –∏–º–µ–Ω–Ω–æ –µ–≥–æ –≤–∞–∂–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å: –≤ –ø—Ä–µ—Ç—Ä–µ–π–Ω–µ –ø–æ–ø–∞–¥–∞–Ω–∏–µ –ø–ª–æ—Ö–∏—Ö –¥–∞–Ω–Ω—ã—Ö —É—Ö—É–¥—à–∞–µ—Ç –º–æ–¥–µ–ª—å. –ü–æ—ç—Ç–æ–º—É –º–µ—Ç—Ä–∏–∫–∞ —Å—Ç—Ä–æ–∏—Ç—Å—è –ø–æ F1 –∫–ª–∞—Å—Å–∞ 0: –≤–∞–∂–Ω–æ –∏ –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –º—É—Å–æ—Ä (recall), –∏ –Ω–µ –≤—ã–±—Ä–∞—Å—ã–≤–∞—Ç—å –ª–∏—à–Ω–∏–µ —Ö–æ—Ä–æ—à–∏–µ —Ç–µ–∫—Å—Ç—ã (precision). –í loss –∑–∞–∫–ª–∞–¥—ã–≤–∞–µ–º –∏–º–µ–Ω–Ω–æ F1 –¥–ª—è –∫–ª–∞—Å—Å–∞ 0.\n",
        "\n",
        "---\n",
        "\n",
        "### Precision vs Recall: –±–∏–∑–Ω–µ—Å-–≤—ã–±–æ—Ä\n",
        "\n",
        "–¢–æ—Ä–≥ –º–µ–∂–¥—É precision –∏ recall –¥–ª—è –∫–ª–∞—Å—Å–∞ 0 –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫ –º—ã —Ñ–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ:\n",
        "\n",
        "- **–í—ã—Å–æ–∫–∏–π recall, –Ω–∏–∂–µ precision**  \n",
        "  –§–∏–ª—å—Ç—Ä—É–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ: –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –±–æ–ª—å—à–µ ¬´—Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö¬ª, –≤ —Ç.—á. —á–∞—Å—Ç—å —Ö–æ—Ä–æ—à–∏—Ö. –ú–µ–Ω—å—à–µ –ø–ª–æ—Ö–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–ø–∞–¥–∞–µ—Ç –≤ –ø—Ä–µ—Ç—Ä–µ–π–Ω, –Ω–æ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –≤—ã—à–µ.\n",
        "\n",
        "- **–í—ã—Å–æ–∫–∏–π precision, –Ω–∏–∂–µ recall**  \n",
        "  –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–µ–µ: —É–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, –≤ —á—ë–º —É–≤–µ—Ä–µ–Ω—ã. –ë–æ–ª—å—à–µ —Ö–æ—Ä–æ—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è, –Ω–æ —á–∞—Å—Ç—å –º—É—Å–æ—Ä–∞ –ø—Ä–æ—Ö–æ–¥–∏—Ç –≤ –¥–∞—Ç–∞—Å–µ—Ç.\n",
        "\n",
        "–í—ã–±–æ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ü–µ–ª–∏: –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å ¬´–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ¬ª –º–æ–¥–µ–ª–∏ (–≤—ã—à–µ recall) –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞–∫—Å–∏–º—É–º –ø–æ–ª–µ–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–≤—ã—à–µ precision). –í loss –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –≤–µ—Å–∞ precision/recall (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ Œ≤ –≤ FŒ≤) –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞.\n",
        "\n",
        "---\n",
        "\n",
        "### –î–∞–ª—å–Ω–µ–π—à–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤\n",
        "\n",
        "–ü—Ä–∏ –±–æ–ª—å—à–µ–º –±—é–¥–∂–µ—Ç–µ (GPU, –≤—Ä–µ–º—è) –∏–º–µ–µ—Ç —Å–º—ã—Å–ª:\n",
        "\n",
        "- **–ö—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏** (–Ω–∞–ø—Ä–∏–º–µ—Ä, rubert-large, XLM-R large) ‚Äî –ª—É—á—à–µ —É–ª–∞–≤–ª–∏–≤–∞—é—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É –∏ –≥—Ä–∞–Ω–∏—Ü—ã –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏, –¥–∞—é—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç —Å–∫–æ—Ä.\n",
        "- **–ë–æ–ª–µ–µ –¥–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ** —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏ –∏ early stopping ‚Äî –º–æ–∂–Ω–æ –ª—É—á—à–µ –ø–æ–¥–æ–±—Ä–∞—Ç—å —á–∏—Å–ª–æ —ç–ø–æ—Ö –∏ —Å–Ω–∏–∑–∏—Ç—å —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.\n",
        "- **–ê–Ω—Å–∞–º–±–ª–∏** (–Ω–µ—Å–∫–æ–ª—å–∫–æ BERT + —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ—ë–≤) ‚Äî –æ–±—ã—á–Ω–æ –¥–∞—é—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏ –≤—ã—Å–æ–∫–∏–π —Å—Ä–µ–¥–Ω–∏–π F1.\n",
        "\n",
        "---\n",
        "\n",
        "> üìÅ **P.S.** –í—Å–µ —ç—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã (–∏ –Ω–µ —Ç–æ–ª—å–∫–æ) –µ—Å—Ç—å –≤ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–º –Ω–æ—É—Ç–±—É–∫–µ `Untitled1_(1).ipynb`. –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—é: —Ç–∞–º ~187 —è—á–µ–µ–∫, –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ ¬´–∑–∞–ø—É—Å—Ç–∏–ª –∏ –ø–æ–±–µ–∂–∞–ª¬ª + –≥–æ–≤–Ω–æ–∫–æ–¥, —á—Ç–æ —è —Å–∞–º —á–µ—Ä–µ–∑ –º–µ—Å—è—Ü –≤ –Ω—ë–º –Ω–µ —Ä–∞–∑–±–µ—Ä—É—Ç—Å—è. –ù–æ –∑–∞—Ç–æ –∫–∞–∂–¥–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ ‚Äî –≤ –Ω–∞–ª–∏—á–∏–∏. üëÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: Colab ‚Äî –º–æ–Ω—Ç–∏—Ä—É–µ–º Drive; –ª–æ–∫–∞–ª—å–Ω–æ ‚Äî —Ç–µ–∫—É—â–∞—è –ø–∞–ø–∫–∞\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATA_PATH = '/content/drive/MyDrive/llm_filter_data'\n",
        "except ImportError:\n",
        "    DATA_PATH = '.'  # –ª–æ–∫–∞–ª—å–Ω–æ: train.parquet –∏ test.parquet –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "train_df = pd.read_parquet(os.path.join(DATA_PATH, 'train.parquet'))\n",
        "test_df = pd.read_parquet(os.path.join(DATA_PATH, 'test.parquet'))\n",
        "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ 0 –∏ 1 (–ø—Ä–∏–º–µ—Ä—ã —Å 0.5 –Ω–µ —É—á–∞—Å—Ç–≤—É—é—Ç –≤ –º–µ—Ç—Ä–∏–∫–µ)\n",
        "train_clean = train_df[\n",
        "    (train_df['integrity'].isin([0, 1])) &\n",
        "    (train_df['factuality'].isin([0, 1])) &\n",
        "    (train_df['truthfulness'].isin([0, 1]))\n",
        "].copy()\n",
        "\n",
        "strat_cols = train_clean[['integrity', 'factuality', 'truthfulness']].apply(\n",
        "    lambda x: x.astype(str).str.cat(), axis=1\n",
        ")\n",
        "train_part, val_part = train_test_split(\n",
        "    train_clean,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=strat_cols\n",
        ")\n",
        "print(f\"Train: {len(train_part)}, Val: {len(val_part)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "def get_remove_columns(dataset):\n",
        "    return [col for col in dataset.column_names if col != 'text']\n",
        "\n",
        "# –¢—Ä–µ–π–Ω\n",
        "train_ds = Dataset.from_pandas(train_part[['text']].copy(), preserve_index=False)\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=get_remove_columns(train_ds))\n",
        "train_tok = train_tok.add_column('integrity', train_part['integrity'].tolist())\n",
        "train_tok = train_tok.add_column('factuality', train_part['factuality'].tolist())\n",
        "train_tok = train_tok.add_column('truthfulness', train_part['truthfulness'].tolist())\n",
        "\n",
        "# –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "val_ds = Dataset.from_pandas(val_part[['text']].copy(), preserve_index=False)\n",
        "val_tok = val_ds.map(tokenize_fn, batched=True, remove_columns=get_remove_columns(val_ds))\n",
        "val_tok = val_tok.add_column('integrity', val_part['integrity'].tolist())\n",
        "val_tok = val_tok.add_column('factuality', val_part['factuality'].tolist())\n",
        "val_tok = val_tok.add_column('truthfulness', val_part['truthfulness'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiTaskDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(item['input_ids']),\n",
        "            'attention_mask': torch.tensor(item['attention_mask']),\n",
        "            'integrity': torch.tensor(item['integrity']),\n",
        "            'factuality': torch.tensor(item['factuality']),\n",
        "            'truthfulness': torch.tensor(item['truthfulness'])\n",
        "        }\n",
        "\n",
        "train_multitask = MultiTaskDataset(train_tok)\n",
        "val_multitask = MultiTaskDataset(val_tok)\n",
        "print(f\"Train: {len(train_multitask)}, Val: {len(val_multitask)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. –ú–æ–¥–µ–ª—å: Multi-Task BERT —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class F1Loss(nn.Module):\n",
        "    def __init__(self, beta=1.0, eps=1e-7):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, logits, targets, pos_label=0):\n",
        "        probs = torch.softmax(logits, dim=1)[:, pos_label]\n",
        "        binary_targets = (targets == pos_label).float()\n",
        "        tp = (probs * binary_targets).sum()\n",
        "        fp = (probs * (1 - binary_targets)).sum()\n",
        "        fn = ((1 - probs) * binary_targets).sum()\n",
        "        precision = tp / (tp + fp + self.eps)\n",
        "        recall = tp / (tp + fn + self.eps)\n",
        "        f1 = (1 + self.beta**2) * precision * recall / (self.beta**2 * precision + recall + self.eps)\n",
        "        return 1 - f1\n",
        "\n",
        "CLASS_WEIGHTS = {\n",
        "    'integrity': torch.tensor([1.74, 0.26]),\n",
        "    'factuality': torch.tensor([1.67, 0.33]),\n",
        "    'truthfulness': torch.tensor([1.45, 0.55])\n",
        "}\n",
        "\n",
        "class MultiTaskBERT(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=2, weights=None, unfreeze=2):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "        for layer in self.bert.encoder.layer[-unfreeze:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "        for param in self.bert.pooler.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.weights = weights or {k: torch.tensor([1.0, 1.0]) for k in ['integrity', 'factuality', 'truthfulness']}\n",
        "        hidden = self.bert.config.hidden_size\n",
        "        self.head_integrity = nn.Sequential(nn.Dropout(0.3), nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_labels))\n",
        "        self.head_factuality = nn.Sequential(nn.Dropout(0.3), nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_labels))\n",
        "        self.head_truthfulness = nn.Sequential(nn.Dropout(0.3), nn.Linear(hidden, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_labels))\n",
        "        self.f1_loss = F1Loss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, integrity=None, factuality=None, truthfulness=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        pooled = outputs.pooler_output\n",
        "        logits_i = self.head_integrity(pooled)\n",
        "        logits_f = self.head_factuality(pooled)\n",
        "        logits_t = self.head_truthfulness(pooled)\n",
        "\n",
        "        loss = None\n",
        "        if all(l is not None for l in [integrity, factuality, truthfulness]):\n",
        "            device = integrity.device\n",
        "            f1_i = self.f1_loss(logits_i, integrity.long(), pos_label=0)\n",
        "            f1_f = self.f1_loss(logits_f, factuality.long(), pos_label=0)\n",
        "            f1_t = self.f1_loss(logits_t, truthfulness.long(), pos_label=0)\n",
        "            w_i = self.weights['integrity'].to(device)\n",
        "            w_f = self.weights['factuality'].to(device)\n",
        "            w_t = self.weights['truthfulness'].to(device)\n",
        "            ce_i = nn.CrossEntropyLoss(weight=w_i)(logits_i, integrity.long())\n",
        "            ce_f = nn.CrossEntropyLoss(weight=w_f)(logits_f, factuality.long())\n",
        "            ce_t = nn.CrossEntropyLoss(weight=w_t)(logits_t, truthfulness.long())\n",
        "            loss_i = 0.7 * f1_i + 0.3 * ce_i\n",
        "            loss_f = 0.7 * f1_f + 0.3 * ce_f\n",
        "            loss_t = 0.7 * f1_t + 0.3 * ce_t\n",
        "            loss = (loss_i + loss_f + loss_t) / 3\n",
        "\n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'logits_integrity': logits_i,\n",
        "            'logits_factuality': logits_f,\n",
        "            'logits_truthfulness': logits_t\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –ö–∞—Å—Ç–æ–º–Ω—ã–π Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels_i = inputs.pop(\"integrity\")\n",
        "        labels_f = inputs.pop(\"factuality\")\n",
        "        labels_t = inputs.pop(\"truthfulness\")\n",
        "        outputs = model(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            integrity=labels_i,\n",
        "            factuality=labels_f,\n",
        "            truthfulness=labels_t\n",
        "        )\n",
        "        return (outputs['loss'], outputs) if return_outputs else outputs['loss']\n",
        "\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
        "        dataloader = self.get_eval_dataloader(eval_dataset)\n",
        "        total_loss, num_batches = 0, 0\n",
        "        preds_i, labels_i = [], []\n",
        "        preds_f, labels_f = [], []\n",
        "        preds_t, labels_t = [], []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"–û—Ü–µ–Ω–∫–∞\"):\n",
        "                batch = {k: v.to(self.args.device) for k, v in batch.items()}\n",
        "                l_i = batch.pop(\"integrity\")\n",
        "                l_f = batch.pop(\"factuality\")\n",
        "                l_t = batch.pop(\"truthfulness\")\n",
        "                outputs = self.model(**batch, integrity=l_i, factuality=l_f, truthfulness=l_t)\n",
        "                if outputs['loss'] is not None:\n",
        "                    total_loss += outputs['loss'].item()\n",
        "                    num_batches += 1\n",
        "                preds_i.extend(torch.argmax(outputs['logits_integrity'], dim=1).cpu().numpy())\n",
        "                labels_i.extend(l_i.cpu().numpy())\n",
        "                preds_f.extend(torch.argmax(outputs['logits_factuality'], dim=1).cpu().numpy())\n",
        "                labels_f.extend(l_f.cpu().numpy())\n",
        "                preds_t.extend(torch.argmax(outputs['logits_truthfulness'], dim=1).cpu().numpy())\n",
        "                labels_t.extend(l_t.cpu().numpy())\n",
        "        metrics = {\n",
        "            f'{metric_key_prefix}_loss': total_loss / num_batches if num_batches else 0,\n",
        "            f'{metric_key_prefix}_integrity_f1_0': f1_score(labels_i, preds_i, pos_label=0, average='binary'),\n",
        "            f'{metric_key_prefix}_factuality_f1_0': f1_score(labels_f, preds_f, pos_label=0, average='binary'),\n",
        "            f'{metric_key_prefix}_truthfulness_f1_0': f1_score(labels_t, preds_t, pos_label=0, average='binary'),\n",
        "        }\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. –û–±—É—á–µ–Ω–∏–µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MultiTaskBERT(MODEL_NAME, weights=CLASS_WEIGHTS, unfreeze=2)\n",
        "print(f\"–û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='./model_output',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=0.1,\n",
        "    logging_steps=50,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='no',\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    disable_tqdm=False,\n",
        "    report_to='none',\n",
        "    dataloader_num_workers=2\n",
        ")\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_multitask,\n",
        "    eval_dataset=val_multitask\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = next(model.parameters()).device\n",
        "val_loader = torch.utils.data.DataLoader(val_multitask, batch_size=16, shuffle=False)\n",
        "all_preds = {'integrity': [], 'factuality': [], 'truthfulness': []}\n",
        "all_labels = {'integrity': [], 'factuality': [], 'truthfulness': []}\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"–í–∞–ª–∏–¥–∞—Ü–∏—è\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        labels_i = batch.pop('integrity')\n",
        "        labels_f = batch.pop('factuality')\n",
        "        labels_t = batch.pop('truthfulness')\n",
        "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        all_preds['integrity'].extend(torch.argmax(outputs['logits_integrity'], dim=1).cpu().numpy())\n",
        "        all_preds['factuality'].extend(torch.argmax(outputs['logits_factuality'], dim=1).cpu().numpy())\n",
        "        all_preds['truthfulness'].extend(torch.argmax(outputs['logits_truthfulness'], dim=1).cpu().numpy())\n",
        "        all_labels['integrity'].extend(labels_i.cpu().numpy())\n",
        "        all_labels['factuality'].extend(labels_f.cpu().numpy())\n",
        "        all_labels['truthfulness'].extend(labels_t.cpu().numpy())\n",
        "\n",
        "f1_scores = []\n",
        "for task in ['integrity', 'factuality', 'truthfulness']:\n",
        "    f1_0 = f1_score(all_labels[task], all_preds[task], pos_label=0, average='binary')\n",
        "    f1_1 = f1_score(all_labels[task], all_preds[task], pos_label=1, average='binary')\n",
        "    f1_scores.append(f1_0)\n",
        "    print(f\"{task}: F1@0={f1_0:.4f}, F1@1={f1_1:.4f}\")\n",
        "print(f\"\\n–°—Ä–µ–¥–Ω–∏–π F1 (–ø–æ –∫–ª–∞—Å—Å—É 0): {np.mean(f1_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–µ –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ds = Dataset.from_pandas(test_df[['text']].copy(), preserve_index=False)\n",
        "remove_cols = [c for c in test_ds.column_names if c != 'text']\n",
        "test_tok = test_ds.map(tokenize_fn, batched=True, remove_columns=remove_cols)\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        return {'input_ids': torch.tensor(item['input_ids']), 'attention_mask': torch.tensor(item['attention_mask'])}\n",
        "\n",
        "test_dataset = TestDataset(test_tok)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "preds_integrity, preds_factuality, preds_truthfulness = [], [], []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"–ò–Ω—Ñ–µ—Ä–µ–Ω—Å\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        preds_integrity.extend(torch.argmax(outputs['logits_integrity'], dim=1).cpu().numpy())\n",
        "        preds_factuality.extend(torch.argmax(outputs['logits_factuality'], dim=1).cpu().numpy())\n",
        "        preds_truthfulness.extend(torch.argmax(outputs['logits_truthfulness'], dim=1).cpu().numpy())\n",
        "\n",
        "id_col = 'uuid' if 'uuid' in test_df.columns else ('id' if 'id' in test_df.columns else None)\n",
        "uuid_values = test_df[id_col].astype(str) if id_col else test_df.index.astype(str)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'uuid': uuid_values,\n",
        "    'integrity': preds_integrity,\n",
        "    'truthfulness': preds_truthfulness,\n",
        "    'factuality': preds_factuality\n",
        "})\n",
        "\n",
        "submission_path = os.path.join(DATA_PATH, 'submission.csv')\n",
        "submission.to_csv(submission_path, index=False)\n",
        "print(f\"Submission —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {submission_path}\")\n",
        "submission.head()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
